{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a212d61-2838-4cf7-af44-bc60cdd62739",
   "metadata": {},
   "source": [
    "# Programming Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916e472-3b5c-4588-b12e-e3d869b2db05",
   "metadata": {},
   "source": [
    "## 1. Binary Classification on Text Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1bd9b-b565-44e1-8aa4-46a6f9f8996e",
   "metadata": {},
   "source": [
    "### (a) Download the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eade2245-71c9-4b5d-8ee3-3cec990bb54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 7613\n",
      "Number of test samples: 3263\n",
      "Percentage of real disaster tweets: 42.96597924602653\n",
      "Percentage of non disaster tweets: 57.03402075397347\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Number of training samples:\", train.shape[0])\n",
    "print(\"Number of test samples:\", test.shape[0])\n",
    "\n",
    "# Calculate the percentage of real disaster tweets\n",
    "disaster_P = (train['target'].value_counts(normalize=True) * 100)\n",
    "print(\"Percentage of real disaster tweets:\", disaster_P[1])\n",
    "print(\"Percentage of non disaster tweets:\", disaster_P[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b08a23-adcc-44ce-a37d-03cf8f81ad81",
   "metadata": {},
   "source": [
    "### (b) Split the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5e118a5f-a763-49a5-9bda-21adce95362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_set, dev_set = train_test_split(train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e7185-31c2-4542-bdf8-722b630b59ab",
   "metadata": {},
   "source": [
    "### (c) Preprocess the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4252597d-e4c7-48b6-bc1d-90786a03185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()    \n",
    "    \n",
    "    # Remove punctuation, @, and URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Lemmatize words\n",
    "    tokens = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = \" \".join([word for word in lemmatized_words if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to both train and dev sets\n",
    "train_set['text'] = train_set['text'].apply(preprocess_text)\n",
    "dev_set['text'] = dev_set['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1568793-d813-41bb-b21d-c6800086d1c7",
   "metadata": {},
   "source": [
    "### (d) Bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f34c2ce1-67a2-40e0-9a66-19e48d87a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3061\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the vectorizer with min_df = 3\n",
    "M = 3\n",
    "count_vec = CountVectorizer(binary=True, min_df=M)\n",
    "X_train = count_vec.fit_transform(train_set['text']).toarray()\n",
    "X_dev = count_vec.transform(dev_set['text']).toarray()\n",
    "\n",
    "print(\"Number of features:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f57f8-7487-4031-9f4c-4dc2ccb8af51",
   "metadata": {},
   "source": [
    "### (e) Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d7110ae-c985-44c1-835b-289a5c1d804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on training set (no regularization): 0.9792168015751477\n",
      "F1 Score on development set (no regularization): 0.6848484848484848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = LogisticRegression(penalty=None,  max_iter=500, random_state=42)\n",
    "model.fit(X_train, train_set['target'])\n",
    "# Report performance on both the training and dev set\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_dev_pred = model.predict(X_dev)\n",
    "f1_train = f1_score(train_set['target'], y_train_pred)\n",
    "f1_dev = f1_score(dev_set['target'], y_dev_pred)\n",
    "print(f\"F1 Score on training set (no regularization): {f1_train}\")\n",
    "print(f\"F1 Score on development set (no regularization): {f1_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "893f9b8a-a67e-4d69-961e-68288ee18616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on training set with L1 : 0.8481071098799631\n",
      "F1 Score on development set with L1: 0.743013698630137\n"
     ]
    }
   ],
   "source": [
    "# Training Logistic Regression with L1 regularization\n",
    "lr_l1 = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "lr_l1.fit(X_train, train_set['target'])\n",
    "\n",
    "# Evaluate F1 score on both training and development sets\n",
    "y_train_pred_l1 = lr_l1.predict(X_train)\n",
    "y_dev_pred_l1 = lr_l1.predict(X_dev)\n",
    "f1_train_l1 = f1_score(train_set['target'], y_train_pred_l1)\n",
    "f1_dev_l1 = f1_score(dev_set['target'], y_dev_pred_l1)\n",
    "print(\"F1 Score on training set with L1 :\", f1_train_l1)\n",
    "print(\"F1 Score on development set with L1:\", f1_dev_l1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ca6496bb-51ee-41c9-b1ac-4deefba07590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on training set (L2 regularization): 0.8896\n",
      "F1 Score on development set (L2 regularization): 0.7501360914534567\n"
     ]
    }
   ],
   "source": [
    "# Training Logistic Regression with L2 regularization\n",
    "lr_l2 = LogisticRegression(penalty='l2', random_state=42)\n",
    "lr_l2.fit(X_train, train_set['target'])\n",
    "\n",
    "# Evaluate F1 score on both training and development sets\n",
    "y_train_pred_l2 = lr_l2.predict(X_train)\n",
    "y_dev_pred_l2 = lr_l2.predict(X_dev)\n",
    "f1_train_l2 = f1_score(train_set['target'], y_train_pred_l2)\n",
    "f1_dev_l2 = f1_score(dev_set['target'], y_dev_pred_l2)\n",
    "print(\"F1 Score on training set (L2 regularization):\", f1_train_l2)\n",
    "print(\"F1 Score on development set (L2 regularization):\", f1_dev_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "793cfc44-e43d-4ba8-944d-44b4a11ef7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most important words:\n",
      "word: 'hiroshima'   coefficient: 3.4346707499188924\n",
      "word: 'spill'       coefficient: 3.3984058462691933\n",
      "word: 'mh370'       coefficient: 3.322315652744539\n",
      "word: 'airport'     coefficient: 3.228504789630286\n",
      "word: 'derailment'  coefficient: 3.216282178210582\n",
      "word: 'typhoon'     coefficient: 3.145955359880152\n",
      "word: 'migrant'     coefficient: 3.133761583749795\n",
      "word: 'wildfire'    coefficient: 3.0836921168088325\n",
      "word: 'earthquake'  coefficient: 2.8840924642956045\n",
      "word: 'crew'        coefficient: 2.5972241151595705\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Find the 10 most important words that determines a disaster\n",
    "feature_names = count_vec.get_feature_names_out()\n",
    "coefficients = lr_l1.coef_[0]\n",
    "top_indices = np.argsort(np.abs(coefficients))[-10:] \n",
    "\n",
    "print(\"Top 10 most important words:\")\n",
    "for index in reversed(top_indices): \n",
    "    word = feature_names[index]\n",
    "    coef = coefficients[index]\n",
    "    print(f\"word: '{word}' {(10 - len(word)) * ' '} coefficient: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d3e9f-37d8-46d6-b9b7-53b1b6082d38",
   "metadata": {},
   "source": [
    "### (f) Bernoulli Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "23a7d317-5fcc-4d9d-bdef-82f7219bf130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Bernoulli Naive Bayes model: 0.7526881720430108\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class BernoulliNB:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.class_log_pri = None\n",
    "        self.feature_log_prob = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        _, num_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        num_classes = len(self.classes)\n",
    "\n",
    "        feature_count = np.zeros((num_classes, num_features))\n",
    "        class_count = np.zeros(num_classes)\n",
    "        \n",
    "        # Calculate class counts and feature counts for each class\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            X_class = X[y == cls]\n",
    "            feature_count[i, :] = X_class.sum(axis=0)\n",
    "            class_count[i] = X_class.shape[0]\n",
    "\n",
    "        # Calculate log probabilities with Laplace smoothing\n",
    "        self.feature_log_prob = np.log((feature_count + self.alpha) / (class_count[:, None] + 2 * self.alpha))\n",
    "        self.class_log_pri = np.log(class_count / y.shape[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_likelihood = X @ self.feature_log_prob.T + (1 - X) @ (np.log(1 - np.exp(self.feature_log_prob))).T\n",
    "        log_prob = log_likelihood + self.class_log_pri\n",
    "        return self.classes[np.argmax(log_prob, axis=1)]\n",
    "\n",
    "model_BNB = BernoulliNB(alpha=1.0)\n",
    "model_BNB.fit(X_train, train_set['target'])\n",
    "y_pred_BNB = model_BNB.predict(X_dev)\n",
    "\n",
    "f1_BNB = f1_score(dev_set['target'], y_pred_BNB)\n",
    "print(\"F1 Score for Bernoulli Naive Bayes model:\", f1_BNB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556bfee-0dc1-4fca-9622-5c007e568872",
   "metadata": {},
   "source": [
    "### (h) N-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0f1363b1-33d7-4be5-9db7-a930aaead41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 2-grams in the vocabulary: 3527\n",
      "10 example 2-grams from the vocabulary:\n",
      "['010401 utc20150805' '10 year' '10 yr' '101 cook' '1030 pm' '10401 utc'\n",
      " '109 sn' '10km maximum' '10th death' '11 charged']\n",
      "F1 Score of development set for Logistic Regression: 0.5644599303135889\n",
      "F1 Score of training set for Logistic Regression: 0.7212317666126418\n",
      "F1 Score on development set for Bernoulli Naive Bayes: 0.48259860788863107\n",
      "F1 Score on training set for Bernoulli Naive Bayes: 0.614521841794569\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create 2-grams using CountVectorizer\n",
    "M = 2  # Setting the threshold for minimum document frequency\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), min_df=M, binary=True)\n",
    "X_train_ngrams = vectorizer.fit_transform(train_set['text']).toarray()\n",
    "X_dev_ngrams = vectorizer.transform(dev_set['text']).toarray()\n",
    "\n",
    "# Report the total number of 2-grams in the vocabulary\n",
    "print(f\"Total number of 2-grams in the vocabulary: {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Display 10 example 2-grams from the vocabulary\n",
    "print(\"10 example 2-grams from the vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "# Train Logistic Regression on 2-grams (Reusing code from 1e)\n",
    "lr_ngram = LogisticRegression(penalty='l2', random_state=42)\n",
    "lr_ngram.fit(X_train_ngrams, train_set['target'])\n",
    "\n",
    "y_pred_lr = lr_ngram.predict(X_dev_ngrams)\n",
    "f1_lr = f1_score(dev_set['target'], y_pred_lr)\n",
    "print(\"F1 Score of development set for Logistic Regression:\", f1_lr)\n",
    "\n",
    "y_pred_lr_train = lr_ngram.predict(X_train_ngrams)\n",
    "f1_lr_train = f1_score(train_set['target'], y_pred_lr_train)\n",
    "print(\"F1 Score of training set for Logistic Regression:\", f1_lr_train)\n",
    "\n",
    "# Train Bernoulli Naive Bayes on 2-grams (Reusing code from 1f)\n",
    "bnb_ngram = BernoulliNB(alpha=1.0)\n",
    "bnb_ngram.fit(X_train_ngrams, train_set['target'])\n",
    "\n",
    "\n",
    "y_pred_bnb = bnb_ngram.predict(X_dev_ngrams)\n",
    "f1_bnb = f1_score(dev_set['target'], y_pred_bnb)\n",
    "print(\"F1 Score on development set for Bernoulli Naive Bayes:\", f1_bnb)\n",
    "\n",
    "y_pred_bnb_train = bnb_ngram.predict(X_train_ngrams)\n",
    "f1_bnb_train = f1_score(train_set['target'], y_pred_bnb_train)\n",
    "print(\"F1 Score on training set for Bernoulli Naive Bayes:\", f1_bnb_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6a4fe58f-9394-44e3-a23e-897711837e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pd.concat([train_set, dev_set])\n",
    "\n",
    "# Build feature vectors not using n-grams method\n",
    "vectorizer_full = CountVectorizer(min_df=4, binary=True)\n",
    "X_full_train = vectorizer_full.fit_transform(full_train['text']).toarray()\n",
    "X_test = vectorizer_full.transform(test['text']).toarray()  \n",
    "\n",
    "# Train using BernoulliNaiveBayes Model\n",
    "bnb = BernoulliNB(alpha=1.0)\n",
    "bnb.fit(X_full_train, full_train_data['target'])\n",
    "\n",
    "y_test_pred = bnb.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'id': test['id'], 'target': y_test_pred})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
